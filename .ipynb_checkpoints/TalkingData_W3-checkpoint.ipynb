{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 3 - Model Tuning\n",
    "\n",
    "This week we are going to learn how to use Keras to create Multlayer Peceptron (MLP) models and how to tune the models.\n",
    "\n",
    "We will use the datasets we created last week which, however, may need to be massaged a bit to work for Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "import random\n",
    "from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "from keras.layers.core import Activation\n",
    "from keras.utils import np_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "print(\"# Load Phone Brand\")\n",
    "phone_brand = pd.read_csv(\"../input/phone_brand_device_model.csv\",\n",
    "                  dtype={'device_id': np.str})\n",
    "phone_brand.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "print(\"# Load Train and Test\")\n",
    "train_data = pd.read_csv(\"../input/gender_age_train.csv\",\n",
    "                    dtype={'device_id': np.str})\n",
    "\n",
    "test_data = pd.read_csv(\"../input/gender_age_test.csv\",\n",
    "                   dtype={'device_id': np.str})\n",
    "\n",
    "\n",
    "full_data = pd.concat((train_data, test_data), axis=0, ignore_index=True)\n",
    "train_size = len(train_data)\n",
    "full_data = pd.merge(full_data, phone_brand, how='left',\n",
    "                on='device_id', left_index=True)\n",
    "\n",
    "print (\"Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Group columns - categorical, numerical, target and id\n",
    "data_types = full_data.dtypes  \n",
    "\n",
    "#ID\n",
    "id_col = 'device_id'\n",
    "#Target\n",
    "target_col = 'group'\n",
    "\n",
    "#Categorical columns:\n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "cat_cols.remove('group')\n",
    "cat_cols.remove('gender')\n",
    "cat_cols.remove('device_id')\n",
    "\n",
    "#Numeric columns:\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "num_cols.remove('age')\n",
    "\n",
    "\n",
    "print (\"ID column:\", id_col)\n",
    "print (\"Target column:\",target_col)\n",
    "print (\"Categorical column:\",cat_cols)\n",
    "print (\"Numeric column:\",num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Label target\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "Y=LBL.fit_transform(full_data[target_col][:train_size])\n",
    "Y_labels = np_utils.to_categorical(Y)\n",
    "\n",
    "target_names=LBL.classes_\n",
    "print (\"target group names:\", target_names)\n",
    "\n",
    "full_data['gender']=full_data['gender'].apply(lambda x:1 if x=='F' else 0)\n",
    "\n",
    "device_id = full_data[train_size:][\"device_id\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Aggregate transactional data onto higher granularity **\n",
    "\n",
    "We will load events, app_events, app_labels seperately, then aggregate them by device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "app_ev = pd.read_csv(\"../input/app_events.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "events = pd.read_csv(\"../input/events.csv\", dtype={'device_id': np.str})\n",
    "print (\"Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "app_lab = pd.read_csv(\"../input/app_labels.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Labels loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "lab_cat = pd.read_csv(\"../input/label_categories.csv\", dtype={'device_id': np.str})\n",
    "print (\"Label Categories loaded in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Concatenate applications, labels and label categories to a big text column for each device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device_app = pd.merge(events[['device_id','event_id']]\n",
    "                      , app_ev[['event_id','app_id']], on='event_id')[['device_id','app_id']].drop_duplicates()\n",
    "device_label = pd.merge(device_app\n",
    "                        , app_lab, on='app_id')[['device_id','label_id']].drop_duplicates()\n",
    "device_category= pd.merge(device_label\n",
    "                          , lab_cat, on='label_id')[['device_id','category']].drop_duplicates()\n",
    "print (\"device apps labels and categories aggregated in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Group categoris/labels/apps by device id and merge them into one big list **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device_category = device_category.groupby(\"device_id\")[\"category\"].apply(list)\n",
    "device_label = device_label.groupby(\"device_id\")[\"label_id\"].apply(list)\n",
    "device_app = device_app.groupby(\"device_id\")[\"app_id\"].apply(list)\n",
    "del app_ev,events, lab_cat, app_lab\n",
    "print device_category.shape, device_label.shape, device_app.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data[\"category\"] = full_data[\"device_id\"].map(device_category).apply(\n",
    "    lambda x:' '.join(c for c in x) if x==x else '') \n",
    "full_data[\"label\"] = full_data[\"device_id\"].map(device_label).apply(\n",
    "    lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "full_data[\"app\"] = full_data[\"device_id\"].map(device_app).apply(lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "\n",
    "full_data['device_model'] = full_data['device_model'].apply(lambda x:x.replace(' ','')) \n",
    "full_data['category'] = full_data['category'].apply(lambda x:x.replace(' ','')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** count frequecies of each key word (brand, model and app id), then convert the results to a sparse matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = CountVectorizer(min_df=1)\n",
    "matrix = full_data[[\"phone_brand\", \"device_model\", \"app\",\"label\"]].astype(np.str).apply(\n",
    "    lambda x: \" \".join(s for s in x), axis=1)\n",
    "matrix = counter.fit_transform(matrix)\n",
    "num_of_feature = matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create MLP model with Keras\n",
    "\n",
    "To create a data model we need to define:\n",
    "\n",
    "1. Input Layer\n",
    "    * input_dim = number of features\n",
    "    * activation = relu\n",
    "2. Hidden Layers  \n",
    "    * number of units: let's get started with 512 and tune it later. Typically more units tend to give better performance and take longer time to train. However, the performance/ # of units curve is like a \"U\": at one point more units wouldn't gain more performance\n",
    "    * activation = relu\n",
    "3. Output Layer\n",
    "    * number of units = number of classes, i.e. 12\n",
    "    * activation = softmax, which is ued for multiclasses\n",
    "4. Optimizer \n",
    "    * We will use Nadam to start - you can also try SGD or others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #     Input Layer\n",
    "    model.add(Dense(512, \n",
    "                    input_dim=input_dim,\n",
    "                    activation='relu'))\n",
    "\n",
    "    #     Hidden Layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    #     Output Layer\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    #     Optimizer\n",
    "    nadam = Nadam(lr=1e-4)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=nadam)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert Sparse Matrix to Dense, in batches **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generator for training\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "# generator for predicting            \n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split data for validation **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = matrix[:train_size, :]\n",
    "test = matrix[train_size:, :]\n",
    "num_class = 12\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, Y_labels, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Train and validate the model we defined using early stopping **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "model = create_model(num_of_feature)\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=30,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Add dropout **\n",
    "* Starting with drop rate 0.2, but you can try different rates to see which is the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #     Input Layer\n",
    "    model.add(Dense(512, \n",
    "                    input_dim=input_dim,\n",
    "                    activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #     Hidden Layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #     Output Layer\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    #     Optimizer\n",
    "    nadam = Nadam(lr=1e-4)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=nadam)\n",
    "    return model\n",
    "\n",
    "model = create_model(num_of_feature)\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=30,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tune number of units of hidden layer **\n",
    "* Let's try 256 (1/2 of input units), 341 （2/3 of input units) and 512 (same as input units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #     Input Layer\n",
    "    model.add(Dense(512, \n",
    "                    input_dim=input_dim,\n",
    "                    activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #     Hidden Layer\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #     Output Layer\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    #     Optimizer\n",
    "    nadam = Nadam(lr=1e-4)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=nadam)\n",
    "    return model\n",
    "\n",
    "model = create_model(num_of_feature)\n",
    "fit= model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                         nb_epoch=30,\n",
    "                         samples_per_epoch=train_size,\n",
    "                         validation_data=(X_val.todense(), y_val),\n",
    "                         callbacks=[early_stop]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use the tuned model to create submission **\n",
    "\n",
    "We will train 5 models using the same settings and create a sumbission by simply averaging the predictions. This will give us better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print (\"Training model %d\" % (i+1))\n",
    "    model=create_model(num_of_feature)\n",
    "    fit= model.fit_generator(generator=batch_generator(train, Y_labels, 128, True),\n",
    "                             nb_epoch=<epoch of best model>,\n",
    "                             samples_per_epoch=train.shape[0]\n",
    "                             )\n",
    "    preds=preds+model.predict_generator(generator=batch_generatorp(test, 128, False), val_samples=test.shape[0])\n",
    "    \n",
    "preds = preds/60\n",
    "submission = pd.DataFrame(preds, columns=label_group.classes_)\n",
    "submission[\"device_id\"] = device_id\n",
    "submission = submission.set_index(\"device_id\")\n",
    "submission.to_csv('submission.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework\n",
    "1. Try different activations and optimizers and see the differences in performance.\n",
    "2. Change learning rate to see it effects the traning time and accuracy.\n",
    "3. Change batch size to see it effects the traning time and accuracy.\n",
    "4. If possible, try two hidden layers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
