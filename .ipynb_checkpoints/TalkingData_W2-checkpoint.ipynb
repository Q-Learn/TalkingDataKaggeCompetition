{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 2 - Feature Engineering\n",
    "\n",
    "This week we will be engineering two types of data.\n",
    "\n",
    "Firstly, we will use \"Leave-one-out\" encoding, an approach heavily used by Owen Zhang, former #1 Kaggler, to engineer categorical features. The idea is to count the mean of the target, age and gender for this case, grouping by a given categorical feature, then use the means to replace the original value. Please be advised that this approach can easily lead to overfitting so we'll have to introduce noises to avoid overfitting.\n",
    "\n",
    "Next, we will be working with transactional data. In this competition, there are multiple levels of transactional and hierarchical data. Essentially we will aggregate lower level records to the higher level by concatenating the values of each record into a big text feature. Then count the frequency of each value and convert the results into a sparse matrix. In this way, we can keep as much information as possible without sacrificing the performance thanks to the sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import time\n",
    "import random\n",
    "from sklearn import preprocessing, pipeline, metrics, grid_search, cross_validation\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a new function for Leave-one-out encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_model(train_x, train_y, est, param_grid, n_jobs, cv, refit=False):\n",
    "##Grid Search for the best model\n",
    "    model = grid_search.GridSearchCV(estimator  = est,\n",
    "                                     param_grid = param_grid,\n",
    "                                     scoring    = 'log_loss',\n",
    "                                     verbose    = 10,\n",
    "                                     n_jobs  = n_jobs,\n",
    "                                     iid        = True,\n",
    "                                     refit    = refit,\n",
    "                                     cv      = cv)\n",
    "    # Fit Grid Search Model\n",
    "    model.fit(train_x, train_y)\n",
    "    print(\"Best score: %0.3f\" % model.best_score_)\n",
    "    print(\"Best parameters set:\", model.best_params_)\n",
    "    print(\"Scores:\", model.grid_scores_)\n",
    "    return model\n",
    "\n",
    "def loo_encode(data,cat_col,target_col,train_size,random_rate=0.05):\n",
    "    print (\"Leave-one-out encoding %s on %s\" % (cat_col,target_col))\n",
    "    aggr=data[:train_size].groupby(cat_col)[target_col].agg([np.mean,np.size,np.sum]).reset_index()\n",
    "    data=pd.merge(data,aggr, how='left',on=cat_col)\n",
    "    data['loo']=data['mean']\n",
    "    data['loo'][:train_size]=data[:train_size].apply(lambda row: 0 if row['size']<=1\n",
    "                                                     else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(1-random_rate, 1+random_rate) ,\n",
    "                                                     axis=1).values\n",
    " \n",
    "    return data['loo'].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "print(\"# Load Phone Brand\")\n",
    "phone_brand = pd.read_csv(\"../input/phone_brand_device_model.csv\",\n",
    "                  dtype={'device_id': np.str})\n",
    "phone_brand.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "\n",
    "print(\"# Load Train and Test\")\n",
    "train_data = pd.read_csv(\"../input/gender_age_train.csv\",\n",
    "                    dtype={'device_id': np.str})\n",
    "\n",
    "test_data = pd.read_csv(\"../input/gender_age_test.csv\",\n",
    "                   dtype={'device_id': np.str})\n",
    "\n",
    "\n",
    "full_data = pd.concat((train_data, test_data), axis=0, ignore_index=True)\n",
    "train_size = len(train_data)\n",
    "full_data = pd.merge(full_data, phone_brand, how='left',\n",
    "                on='device_id', left_index=True)\n",
    "\n",
    "print (\"Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Group columns - categorical, numerical, target and id\n",
    "data_types = full_data.dtypes  \n",
    "\n",
    "#ID\n",
    "id_col = 'device_id'\n",
    "#Target\n",
    "target_col = 'group'\n",
    "\n",
    "#Categorical columns:\n",
    "cat_cols = list(data_types[data_types=='object'].index)\n",
    "cat_cols.remove('group')\n",
    "cat_cols.remove('gender')\n",
    "cat_cols.remove('device_id')\n",
    "\n",
    "#Numeric columns:\n",
    "num_cols = list(data_types[data_types=='int64'].index) + list(data_types[data_types=='float64'].index)\n",
    "num_cols.remove('age')\n",
    "\n",
    "\n",
    "print (\"ID column:\", id_col)\n",
    "print (\"Target column:\",target_col)\n",
    "print (\"Categorical column:\",cat_cols)\n",
    "print (\"Numeric column:\",num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Label target\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "Y=LBL.fit_transform(full_data[target_col][:train_size])\n",
    "    \n",
    "target_names=LBL.classes_\n",
    "print (\"target group names:\", target_names)\n",
    "\n",
    "full_data['gender']=full_data['gender'].apply(lambda x:1 if x=='F' else 0)\n",
    "\n",
    "device_id = full_data[train_size:][\"device_id\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Concatenate brand and model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data['brand_model']=full_data['phone_brand']+full_data['device_model']\n",
    "cat_cols.append('brand_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Leave-one-out encode categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loo_cols = []\n",
    "for c in cat_cols:\n",
    "    for t in ['age','gender']:\n",
    "        loo_col=c+'_'+t+'_loo'\n",
    "        full_data[loo_col]=loo_encode(full_data[[c,t]],c,t,train_size,random_rate=0.05)\n",
    "        loo_cols.append(loo_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Use XGBclassifier as baseline **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(full_data[loo_cols].values[:train_size], Y\n",
    "                                                  , train_size=.80, random_state=1234)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred_val=clf.predict_proba(X_val)\n",
    "print (\"mlogloss: %f\" % (metrics.log_loss(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Aggregate transactional data onto higher granularity **\n",
    "\n",
    "We will load events, app_events, app_labels seperately, then aggregate them by device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "app_ev = pd.read_csv(\"../input/app_events.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "events = pd.read_csv(\"../input/events.csv\", dtype={'device_id': np.str})\n",
    "print (\"Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "app_lab = pd.read_csv(\"../input/app_labels.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Labels loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "lab_cat = pd.read_csv(\"../input/label_categories.csv\", dtype={'device_id': np.str})\n",
    "print (\"Label Categories loaded in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Concatenate applications, labels and label categories to a big text column for each device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device_app = pd.merge(events[['device_id','event_id']]\n",
    "                      , app_ev[['event_id','app_id']], on='event_id')[['device_id','app_id']].drop_duplicates()\n",
    "device_label = pd.merge(device_app\n",
    "                        , app_lab, on='app_id')[['device_id','label_id']].drop_duplicates()\n",
    "device_category= pd.merge(device_label\n",
    "                          , lab_cat, on='label_id')[['device_id','category']].drop_duplicates()\n",
    "print (\"device apps labels and categories aggregated in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Group categoris/labels/apps by device id and merge them into one big list **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device_category = device_category.groupby(\"device_id\")[\"category\"].apply(list)\n",
    "device_label = device_label.groupby(\"device_id\")[\"label_id\"].apply(list)\n",
    "device_app = device_app.groupby(\"device_id\")[\"app_id\"].apply(list)\n",
    "del app_ev,events, lab_cat, app_lab\n",
    "print device_category.shape, device_label.shape, device_app.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_data[\"category\"] = full_data[\"device_id\"].map(device_category).apply(\n",
    "    lambda x:' '.join(c for c in x) if x==x else '') \n",
    "full_data[\"label\"] = full_data[\"device_id\"].map(device_label).apply(\n",
    "    lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "full_data[\"app\"] = full_data[\"device_id\"].map(device_app).apply(lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "\n",
    "full_data['device_model'] = full_data['device_model'].apply(lambda x:x.replace(' ','')) \n",
    "full_data['category'] = full_data['category'].apply(lambda x:x.replace(' ','')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** count frequecies of each key word (brand, model and app id), then convert the results to a sparse matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = CountVectorizer(min_df=1)\n",
    "matrix = full_data[[\"phone_brand\", \"device_model\", \"app\"]].astype(np.str).apply(\n",
    "    lambda x: \" \".join(s for s in x), axis=1)\n",
    "matrix = counter.fit_transform(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** XGB baseline - brand, model and application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(matrix[:train_size], Y, train_size=.80, random_state=1234)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred_val=clf.predict_proba(X_val)\n",
    "print (\"mlogloss: %f\" % (metrics.log_loss(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything went smoothly you should see a significant improvement on mlogss from 2.39~ to 2.33~.\n",
    "\n",
    "Now we will use a trick called early stopping to find out the optimal number of iterations for XGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier( n_estimators = 1000, learning_rate=0.3)\n",
    "clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mlogloss',\n",
    "            early_stopping_rounds=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best iteration gained from previous step to re-train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_iteration = clf.best_iteration_\n",
    "best_score=clf.best_score_\n",
    "\n",
    "print (best_iteration, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier( n_estimators = best_iteration, learning_rate=0.3)\n",
    "clf.fit(matrix[:train_size], Y)\n",
    "pred=clf.predict_proba(matrix[train_size:])\n",
    "\n",
    "result = pd.DataFrame(pred, columns=target_names)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('brand_model_app_xgb.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework\n",
    "\n",
    "1. During the aggregation we dropped the duplicates of apps, labels and categories. What if we keep the duplicates? Would that help? Could there be any other aggregation strategies?\n",
    "\n",
    "2. We only used apps in this notebook. Can you also try to labels, categories and different combinations?\n",
    "\n",
    "3. Can we also try the combination of transactinal data and LOO features?\n",
    "\n",
    "4. Is there any other feature engineerings that we can do?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
