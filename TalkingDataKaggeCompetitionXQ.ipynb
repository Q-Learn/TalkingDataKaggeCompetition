{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TalkingData Mobile User Demographics (Kaggle Competition)\n",
    "1. Data preprocessing\n",
    "2. Benchmark models: random forest and naive bayes\n",
    "3. Leave one out encoding\n",
    "4. Hierarchical data of multiple levels \n",
    "5. XGBoost\n",
    "6. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# numpy, scipy, and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "\n",
    "# scikit-learn for machine learning\n",
    "from sklearn import preprocessing, metrics, grid_search, cross_validation#, pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# xgboost and keras\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Nadam\n",
    "from keras.layers.advanced_activations import SReLU\n",
    "from keras.layers.core import Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load Data\n",
    "\n",
    "print(\"# Load Data of Phone Brand and Device Model\")\n",
    "phone_brand = pd.read_csv(\"../input/phone_brand_device_model.csv\", dtype={'device_id': np.str})\n",
    "phone_brand.drop_duplicates('device_id', keep='first', inplace=True)\n",
    "\n",
    "print(\"# Load Training Data\")\n",
    "train_data = pd.read_csv(\"../input/gender_age_train.csv\", dtype={'device_id': np.str})\n",
    "\n",
    "print(\"# Load Testing Data\")\n",
    "test_data = pd.read_csv(\"../input/gender_age_test.csv\", dtype={'device_id': np.str})\n",
    "\n",
    "full_data = pd.concat((train_data, test_data), axis=0, ignore_index=True)\n",
    "train_size = len(train_data)\n",
    "full_data = pd.merge(full_data, phone_brand, how='left', on='device_id', left_index=True)\n",
    "\n",
    "print (\"# Data Loaded.\")\n",
    "full_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# label/encode target\n",
    "LBL = preprocessing.LabelEncoder()\n",
    "Y = LBL.fit_transform(full_data['group'][:train_size])\n",
    "Y_labels = np_utils.to_categorical(Y)\n",
    "\n",
    "target_names = LBL.classes_\n",
    "print (\"target group names:\", target_names)\n",
    "device_id = full_data[train_size:][\"device_id\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Benchmark models: random forest and naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "full_ohe = pd.get_dummies(full_data[['phone_brand', 'device_model']], sparse=True)\n",
    "full_ohe = sparse.csr_matrix(full_ohe)\n",
    "\n",
    "# lable encoding\n",
    "full_le = pd.DataFrame()\n",
    "full_le['phone_brand'] = LBL.fit_transform(full_data['phone_brand'])\n",
    "full_le['device_model'] = LBL.fit_transform(full_data['device_model'])\n",
    "\n",
    "print full_ohe.shape, full_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random forest with label encoding\n",
    "model = grid_search.GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                 param_grid={}, \n",
    "                                 scoring='log_loss',\n",
    "                                 n_jobs=1,\n",
    "                                 iid=True,\n",
    "                                 cv=4, \n",
    "                                 refit=False,\n",
    "                                 verbose=10)\n",
    "model.fit(full_le[:train_size], Y)\n",
    "print (\"grid scores:\", model.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random forest with one hot encoding\n",
    "model = grid_search.GridSearchCV(RandomForestClassifier(n_estimators=100), \n",
    "                                 param_grid={}, \n",
    "                                 scoring='log_loss',\n",
    "                                 n_jobs=1,\n",
    "                                 iid=True,\n",
    "                                 cv=4, \n",
    "                                 refit=False,\n",
    "                                 verbose=10)   \n",
    "model.fit(full_ohe[:train_size], Y)\n",
    "print (\"grid scores:\", model.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# naive bayes with label encoding\n",
    "model = grid_search.GridSearchCV(GaussianNB(), \n",
    "                                 param_grid={}, \n",
    "                                 scoring='log_loss',\n",
    "                                 n_jobs=1,\n",
    "                                 iid=True,\n",
    "                                 cv=4, \n",
    "                                 refit=False,\n",
    "                                 verbose=10)   \n",
    "model.fit(full_le[:train_size], Y)\n",
    "print (\"grid scores:\", model.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3. Leave one out encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function for leave one out encoding\n",
    "def loo_encode(data,cat_col,target_col,train_size,random_rate=0.05):\n",
    "    print (\"leave one out encoding %s on %s\" % (cat_col, target_col))\n",
    "    aggr = data[:train_size].groupby(cat_col)[target_col].agg([np.mean,np.size,np.sum]).reset_index()\n",
    "    data = pd.merge(data, aggr, how='left', on=cat_col)\n",
    "    data['loo'] = data['mean']\n",
    "    data['loo'][:train_size] = data[:train_size].apply(lambda row: 0 if row['size']<=1 \n",
    "        else (row['sum']-row[target_col])/(row['size']-1)*random.uniform(1-random_rate, 1+random_rate), axis=1).values\n",
    " \n",
    "    return data['loo'].fillna(0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# label/encode target \"gender\" \n",
    "full_data['gender'] = full_data['gender'].apply(lambda x:1 if x=='F' else 0)\n",
    "\n",
    "# concatenate \"phone_brand\" and \"device_model\" to create a new categorical feature\n",
    "full_data['brand_model'] = full_data['phone_brand'] + full_data['device_model']\n",
    "cat_cols = ['phone_brand', 'device_model', 'brand_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# leave one out encoding for 3 categorical features on 2 targets\n",
    "loo_cols = []\n",
    "for c in cat_cols:\n",
    "    for t in ['age','gender']:\n",
    "        loo_col=c+'_'+t+'_loo'\n",
    "        full_data[loo_col]=loo_encode(full_data[[c,t]],c,t,train_size,random_rate=0.05)\n",
    "        loo_cols.append(loo_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4. Hierarchical data of multiple levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use XGBclassifier as baseline\n",
    "X_train, X_val, y_train, y_val = train_test_split(full_data[loo_cols].values[:train_size], \n",
    "                                                  Y, \n",
    "                                                  train_size=.80, \n",
    "                                                  random_state=1234)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred_val=clf.predict_proba(X_val)\n",
    "print (\"mlogloss: %f\" % (metrics.log_loss(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data: events, app_events, app_labels, and label_categories\n",
    "\n",
    "start = time.time()\n",
    "events = pd.read_csv(\"../input/events.csv\", dtype={'device_id': np.str})\n",
    "print (\"Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "app_ev = pd.read_csv(\"../input/app_events.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Events loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "app_lab = pd.read_csv(\"../input/app_labels.csv\", dtype={'device_id': np.str})\n",
    "print (\"App Labels loaded in %f seconds\" %(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "lab_cat = pd.read_csv(\"../input/label_categories.csv\", dtype={'device_id': np.str})\n",
    "print (\"Label Categories loaded in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# aggregate apps labels and categories by device\n",
    "device_app = pd.merge(events[['device_id','event_id']], app_ev[['event_id','app_id']], \n",
    "                      on='event_id')[['device_id','app_id']].drop_duplicates()\n",
    "device_label = pd.merge(device_app, app_lab, \n",
    "                        on='app_id')[['device_id','label_id']].drop_duplicates()\n",
    "device_category = pd.merge(device_label, lab_cat, \n",
    "                           on='label_id')[['device_id','category']].drop_duplicates()\n",
    "print (\"device apps labels and categories aggregated in %f seconds\" %(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concatenate applications, labels, and categories to a big text column for each device\n",
    "device_category = device_category.groupby(\"device_id\")[\"category\"].apply(list)\n",
    "device_label = device_label.groupby(\"device_id\")[\"label_id\"].apply(list)\n",
    "device_app = device_app.groupby(\"device_id\")[\"app_id\"].apply(list)\n",
    "del app_ev,events, lab_cat, app_lab\n",
    "print device_category.shape, device_label.shape, device_app.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group categories/labels/apps by device id and merge them into one big list\n",
    "full_data[\"category\"] = full_data[\"device_id\"].map(device_category).apply(\n",
    "    lambda x:' '.join(c for c in x) if x==x else '') \n",
    "full_data[\"label\"] = full_data[\"device_id\"].map(device_label).apply(\n",
    "    lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "full_data[\"app\"] = full_data[\"device_id\"].map(device_app).apply(\n",
    "    lambda x:' '.join(str(c) for c in x) if x==x else '') \n",
    "\n",
    "full_data['device_model'] = full_data['device_model'].apply(lambda x:x.replace(' ','')) \n",
    "full_data['category'] = full_data['category'].apply(lambda x:x.replace(' ','')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count frequecies of each key word (brand, model, and app id), then convert the results to a sparse matrix\n",
    "counter = CountVectorizer(min_df=1)\n",
    "matrix = full_data[[\"phone_brand\", \"device_model\", \"app\"]].astype(np.str).apply(\n",
    "    lambda x: \" \".join(s for s in x), axis=1)\n",
    "matrix = counter.fit_transform(matrix)\n",
    "num_of_feature = matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGB baseline - brand, model, and application\n",
    "X_train, X_val, y_train, y_val = train_test_split(matrix[:train_size], Y, train_size=.80, random_state=1234)\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "pred_val = clf.predict_proba(X_val)\n",
    "print (\"mlogloss: %f\" % (metrics.log_loss(y_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use a trick called early stopping to find out the optimal number of iterations for XGB\n",
    "clf = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.3)\n",
    "clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mlogloss', early_stopping_rounds=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best_iteration = clf.best_iteration_\n",
    "# best_score = clf.best_score_\n",
    "best_iteration = 325\n",
    "best_score = 2.29486\n",
    "\n",
    "print (best_iteration, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create submission\n",
    "clf = xgb.XGBClassifier(n_estimators=best_iteration, learning_rate=0.3)\n",
    "clf.fit(matrix[:train_size], Y)\n",
    "pred = clf.predict_proba(matrix[train_size:])\n",
    "\n",
    "result = pd.DataFrame(pred, columns=target_names)\n",
    "result[\"device_id\"] = device_id\n",
    "result = result.set_index(\"device_id\")\n",
    "result.to_csv('brand_model_app_xgb.csv', index=True, index_label='device_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert sparse matrix to dense (in batches)\n",
    "\n",
    "# generator for training\n",
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator \n",
    "    #https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "# generator for predicting            \n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data for validation\n",
    "train = matrix[:train_size, :]\n",
    "test = matrix[train_size:, :]\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, Y_labels, train_size=.80, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create MLP model with Keras\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    #     Input Layer\n",
    "    model.add(Dense(512, \n",
    "                    input_dim=input_dim,\n",
    "                    activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #     Hidden Layer\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #     Output Layer\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    #     Optimizer\n",
    "    nadam = Nadam(lr=1e-4)\n",
    "\n",
    "    # Compile Model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=nadam)\n",
    "    return model\n",
    "\n",
    "model = create_model(num_of_feature)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "fit = model.fit_generator(generator=batch_generator(X_train, y_train, 128, True),\n",
    "                          nb_epoch=30,\n",
    "                          samples_per_epoch=train_size,\n",
    "                          validation_data=(X_val.todense(), y_val),\n",
    "                          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create submission\n",
    "\n",
    "for i in range(5):\n",
    "    print (\"Training model %d\" % (i+1))\n",
    "    model = create_model(num_of_feature)\n",
    "    fit = model.fit_generator(generator=batch_generator(train, Y_labels, 128, True),\n",
    "                              nb_epoch=<epoch of best model>,\n",
    "                              samples_per_epoch=train.shape[0])\n",
    "    \n",
    "    preds=preds+model.predict_generator(generator=batch_generatorp(test, 128, False), val_samples=test.shape[0])\n",
    "    \n",
    "preds = preds/60\n",
    "submission = pd.DataFrame(preds, columns=label_group.classes_)\n",
    "submission[\"device_id\"] = device_id\n",
    "submission = submission.set_index(\"device_id\")\n",
    "submission.to_csv('brand_model_app_keras.csv', index=True, index_label='device_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
